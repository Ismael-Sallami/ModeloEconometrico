# -*- coding: utf-8 -*-
"""trabajo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11cfLcCa1OdRsAMB-Pj4gUhm1Nd9ehIPr
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = "/content/drive/MyDrive/econometria/ObesityDataSet_raw_and_data_sinthetic.csv"

datos = pd.read_csv(file_path)

datos

"""# Análisis descriptivo"""

pesos = datos['Weight']

"""## Media"""

pesos.mean()

"""## Mínimo"""

pesos.min()

"""## Máximo"""

pesos.max()

"""## Varianza"""

pesos.var()

"""## Desviación típica"""

pesos.std()

pesos.describe()

"""# Gráficos

## Puntos(Peso-Edad)
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))  # Adjust figure size if needed
sns.scatterplot(x='Age', y='Weight', data=datos)
plt.title('Scatter Plot of Age vs Weight')
plt.xlabel('Age')
plt.ylabel('Weight')
plt.show()

"""## Barras(pesos)"""

import seaborn as sns

sns.histplot(pesos, kde=True)

"""## Barras(género)"""

sns.displot(datos["Gender"])

"""Transformamos las variables categóricas"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder

# Crear una copia de los datos para evitar modificar el original
data_cleaned = datos.copy()

# Convertir las columnas categóricas manualmente usando LabelEncoder
label_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC',
              'SMOKE', 'SCC', 'CALC', 'MTRANS', 'NObeyesdad']

le = LabelEncoder()
for col in label_cols:
    data_cleaned[col] = le.fit_transform(data_cleaned[col])


# Revisa el dataset después de la conversión
data_cleaned

import statsmodels.api as sm
import pandas as pd
import numpy as np

# Separa las variables independientes (X) y la dependiente (y)
X = data_cleaned.drop('Weight', axis=1)  # Mantén X como un DataFrame de pandas
y = data_cleaned['Weight']

# Añade una constante para la intersección
X = sm.add_constant(X)

# Verifica si hay valores faltantes o no numéricos y elimínalos
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

X = X.dropna()
y = y.dropna()

# Crea y ajusta el modelo (manteniendo X como DataFrame de pandas)
model = sm.OLS(y, X)
results = model.fit()

# Imprime el resumen del modelo con nombres de las columnas
print(results.summary())

"""# Recta de regresión"""

import seaborn as sns

# Pinta la gráfica de puntos con la recta de regresión
sns.regplot(x=results.fittedvalues, y=y, data=data_cleaned, ci=None, line_kws={"color": "red"})
plt.title('Gráfica de Puntos con Recta de Regresión')
plt.xlabel('Valores Predichos')
plt.ylabel('Valores Reales')
plt.show()

"""# Interpretación de los resultados de la regresión OLS

## 1. Medidas de ajuste del modelo
- **R-squared (R²):** 0.576
  - Esto indica que el modelo explica aproximadamente el 57.6% de la variabilidad en la variable dependiente (y). Aunque es razonable, no es un ajuste extremadamente alto, lo que sugiere que otras variables fuera del modelo podrían influir en el resultado.
  
- **Adjusted R-squared (R² ajustado):** 0.572
  - El R² ajustado es ligeramente inferior al R² simple, ya que penaliza por la inclusión de variables adicionales que no mejoran mucho el ajuste. Esto significa que algunas variables pueden no estar aportando demasiado al modelo.
  
- **F-statistic:** 177.5
  - Este valor evalúa la significancia global del modelo. Un valor F alto con una **probabilidad asociada (Prob F-statistic) de 0.000** indica que el modelo es estadísticamente significativo, es decir, las variables independientes, en conjunto, predicen bien la variable dependiente.

## 2. Coeficientes
Los coeficientes (`coef`) representan el efecto de cada variable independiente en la variable dependiente (y), manteniendo constantes las demás.

- **const (Intersección):** -207.9919
  - Este es el valor de la variable dependiente (y) cuando todas las variables independientes son 0. No tiene una interpretación directa si los valores de las variables \( X \) no pueden ser 0.

- **x1 a x16:**
  - **Significancia de los coeficientes (P>|t|):** Los valores **p** indican si cada variable es estadísticamente significativa para predecir \( y \). Valores de **p** menores a 0.05 sugieren que el coeficiente de esa variable es significativo. La mayoría de las variables aquí son significativas excepto **x9** (con un p-valor de 0.938).
  
  - **Interpretación de los coeficientes:**
    - **x1:** -4.7575 (p-valor = 0.000) → Un aumento en la variable \( x1 \) reduce \( y \) en 4.76 unidades, manteniendo las demás variables constantes.
    - **x3:** 125.2342 (p-valor = 0.000) → \( y \) aumenta en 125.23 unidades por cada incremento unitario en \( x3 \), manteniendo las demás constantes.
    - **x9:** -0.2063 (p-valor = 0.938) → Este coeficiente no es estadísticamente significativo, por lo que \( x9 \) no parece afectar a \( y \) de manera significativa.

## 3. Signos de los coeficientes
- Los coeficientes positivos (como en \( x3 \), \( x4 \), etc.) indican que, a medida que esa variable aumenta, también lo hace la variable dependiente (y).
- Los coeficientes negativos (como \( x1 \), \( x11 \), etc.) indican que, a medida que esa variable aumenta, la variable dependiente (y) disminuye.

## 4. Estadísticas de diagnóstico del modelo
- **Durbin-Watson:** 0.741
  - Esto indica la posible presencia de autocorrelación de los residuos. Un valor cercano a 2 sugiere que no hay autocorrelación. En este caso, 0.741 indica que puede haber correlación positiva entre los errores, lo que podría ser un problema.

- **Omnibus y Jarque-Bera:** Estas pruebas evalúan la normalidad de los residuos. Como el valor p de la prueba Jarque-Bera es muy bajo (**1.94e-05**), esto sugiere que los residuos no siguen una distribución normal.

- **Cond. No. (Número de condición):** 812
  - Un número de condición alto indica la presencia de **multicolinealidad**, es decir, algunas variables independientes están altamente correlacionadas entre sí, lo que puede afectar la estabilidad de los coeficientes.

## Conclusión
El modelo tiene un ajuste moderado (R² = 0.576), con muchas variables que son estadísticamente significativas (excepto \( x9 \)). Sin embargo, los diagnósticos sugieren la posibilidad de autocorrelación de los errores y problemas de multicolinealidad, lo que podría afectar la confiabilidad de las estimaciones. Podrías considerar revisar la multicolinealidad y la autocorrelación para mejorar el modelo.

# Análisis de la multicolienalidad
Se tiene que Cond. No. $ \sqrt{812} = 28,49 $ que como es menor que 30, tenemos unos niveles de multicolienalidad aceptables.

Para calcular el $\text{FIV} = \frac{1}{1-R_i^2}$ para cada una de las variables.

## FIV
"""

import statsmodels.stats.outliers_influence as oi

vifs=[oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vifs

"""## Matriz de correlación"""

# Eliminar la columna 'const' de X
X_no_const = X.loc[:, X.var() > 0]  # Selecciona solo las columnas con varianza distinta de cero

# Calcular la matriz de correlación sin la columna constante
corr_matrix = np.corrcoef(X_no_const.T)
print(corr_matrix)

import statsmodels.graphics.api as smg
import matplotlib.pylab as plt
smg.plot_corr(corr_matrix, xnames=[
    'Gender', 'Age', 'Height', 'family_history_with_overweight', 'FAVC',
    'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
    'CALC', 'MTRANS', 'NObeyesdad'
])
plt.show()
plt.savefig("correlation_plot.pdf", format="pdf", bbox_inches="tight")

"""Vemos que hay multicolienalidad en la variable Height, vamos a eliminarla del modelo"""

import statsmodels.api as sm
import pandas as pd
import numpy as np

# Separa las variables independientes (X) y la dependiente (y)
X = data_cleaned.drop(['Weight', 'Height'], axis=1)  # Excluye 'Height' de las variables independientes
y = data_cleaned['Weight']

# Añade una constante para la intersección
X = sm.add_constant(X)

# Verifica si hay valores faltantes o no numéricos y elimínalos
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

X = X.dropna()
y = y.dropna()

# Crea y ajusta el modelo sin la variable 'Height'
model = sm.OLS(y, X)
results = model.fit()

# Imprime el resumen del modelo sin 'Height'
print(results.summary())

"""Ahora volvemos a calcular la matriz de correlacion para ver si hemos solucionado el problema"""

# Eliminar la columna 'const' de X
X_no_const = X.loc[:, X.var() > 0]  # Selecciona solo las columnas con varianza distinta de cero

# Calcular la matriz de correlación sin la columna constante
corr_matrix = np.corrcoef(X_no_const.T)
smg.plot_corr(corr_matrix, xnames=[
    'Gender', 'Age', 'family_history_with_overweight', 'FAVC',
    'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
    'CALC', 'MTRANS', 'NObeyesdad'
])
plt.show()

"""Ahora vamos a caluclar el fiv y el nc de nuevo para ver la mejora"""

import numpy as np

CN = np.sqrt(results.condition_number) #Número de Condición
print(CN)
# Calcular el número de condición de la matriz de correlación
cond_number = np.linalg.cond(corr_matrix)

# Mostrar el número de condición
print(f'Número de condición de la matriz de correlación: {cond_number}')

import statsmodels.stats.outliers_influence as oi

vifs=[oi.variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vifs

"""Vamos ha hacer otra prueba ahora sin el gender"""

import statsmodels.api as sm
import pandas as pd
import numpy as np

# Separa las variables independientes (X) y la dependiente (y)
X = data_cleaned.drop(['Weight', 'Gender'], axis=1)  # Excluye 'Height' de las variables independientes
y = data_cleaned['Weight']

# Añade una constante para la intersección
X = sm.add_constant(X)

# Verifica si hay valores faltantes o no numéricos y elimínalos
X = X.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

X = X.dropna()
y = y.dropna()

# Crea y ajusta el modelo sin la variable 'Height'
model = sm.OLS(y, X)
results = model.fit()

# Imprime el resumen del modelo sin 'Height'
print(results.summary())

import statsmodels.graphics.api as smg
import matplotlib.pylab as plt
smg.plot_corr(corr_matrix, xnames=[
     'Age',  'family_history_with_overweight', 'FAVC',
    'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
    'CALC', 'MTRANS', 'NObeyesdad'
])
plt.show()

"""# Heteroscedasticidad
## Tests

## Golfeld-Quandt (muestras pequeñas)
"""

import statsmodels.stats.api as sms

#GOLDFELD-QUANDT (Muestras Pequeñas)
#GQ=sms.het_goldfeldquandt(results.resid, results.model.exog)
GQ=sms.het_goldfeldquandt(y, sm.add_constant(datos["Age"])) # Con cada variable
print("Goldfeld Quandt: ", GQ)

"""Esto me dice si las varianzas de los datos pequeños es distinto de los datos grandes.

## Breush-Pagan
"""

#BREUSH-PAGAN
BP=sms.het_breuschpagan(results.resid, results.model.exog)
print("Breush Pagan: ", BP)

"""Tiene que ser menor a 0.05

## White
"""

#WHITE
W=sms.het_white(results.resid, results.model.exog)
print("White: ", W)

"""## Glejser"""

#GLEJSER
import numpy as np
z=np.array(datos["Age"].values, dtype=float)
for h in [-2, -1, -0.5, 0.5, 1, 2]:
    # |e| = delta_0 + delta_1 z^h + eps
    mcoaux=sm.OLS(abs(results.resid), sm.add_constant(z**h)).fit()
    pval=mcoaux.pvalues["x1"]
    print("h: ", h, "-> pvalt:", pval, "R2: ", mcoaux.rsquared)

"""1. Goldfeld-Quandt
Resultado:

Estadístico GQ:
1.2406
1.2406
p-valor:
0.000237
0.000237 (menor a 0.05)
Interpretación:

Dado que el p-valor es menor a 0.05, rechazamos la hipótesis nula de homoscedasticidad. Esto indica que hay evidencia de heteroscedasticidad en los datos, con una varianza creciente (según el resultado "increasing").
2. Breusch-Pagan
Resultado:

Estadístico LM:
310.298
310.298
p-valor:
4.03
×
1
0
−
57
4.03×10
−57
  (muy pequeño, menor a 0.05)
Interpretación:

El p-valor extremadamente bajo sugiere que rechazamos la hipótesis nula de homoscedasticidad. Existe una fuerte evidencia de heteroscedasticidad.
3. White
Resultado:

Estadístico LM:
938.445
938.445
p-valor:
1.40
×
1
0
−
122
1.40×10
−122
  (muy pequeño, menor a 0.05)
Interpretación:

El test de White también indica que rechazamos la hipótesis nula de homoscedasticidad. Esto confirma la presencia de heteroscedasticidad.
4. Glejser
Resultados para diferentes valores de
h
h:

Para
h
=
−
2
,
−
1
,
−
0.5
,
0.5
,
1
,
2
h=−2,−1,−0.5,0.5,1,2, todos los p-valores son extremadamente pequeños (menores a 0.05), lo que indica que para cada transformación del predictor, existe evidencia de heteroscedasticidad.
Los valores de
R
2
R
2
  son bajos, pero consistentes, indicando que la relación entre los errores absolutos y las transformaciones del predictor existe pero no es fuerte.
Interpretación:

Los resultados de Glejser también confirman la presencia de heteroscedasticidad.

## Gráficos
"""

import matplotlib.pylab as plt

plt.scatter(list(range(int(results.nobs))), results.resid**2)
plt.show()
plt.scatter(list(range(int(results.nobs))), results.resid)
plt.show()

import matplotlib.pyplot as plt

variables = ['Gender', 'Age', 'Height', 'family_history_with_overweight', 'FAVC',
             'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
             'CALC', 'MTRANS', 'NObeyesdad']

for variable in variables:
    plt.scatter(datos[variable], results.resid)
    plt.title(f'{variable} vs Res')
    plt.xlabel(variable)  # Add x-axis label
    plt.ylabel('Residuals')  # Add y-axis label
    plt.show()

"""Tan solo se observa algo de heterocedasticidad en la variable Age.

# Mínimos Cuadrados Ponderados
"""

mcp = sm.WLS(y, sm.add_constant(X), weights=1./np.sqrt(z)).fit()
mcp.summary()

"""# Autocorrelación (no tiene mucho sentido con nuestros datos)"""

import matplotlib.pylab as plt

plt.scatter(list(range(int(mcp.nobs))), mcp.resid)
plt.plot([-1,mcp.nobs+1], [0,0], color='r')
plt.title("Observaciones vs Residuos")
plt.show()

"""## Durbin-Watson"""

from statsmodels.stats.stattools import durbin_watson

dw = durbin_watson(mcp.resid)
print("Durbin-Watson statistic:", dw)

"""## H_Durbin"""

import numpy as np
import statsmodels.api as sm
from statsmodels.stats.stattools import durbin_watson

# Simulated example: Autoregressive data
np.random.seed(0)
n = 100
y_original = datos["Weight"]
y_retardada = np.roll(y_original, 1)  #
y_retardada[0] = 0  #


modelo = sm.OLS(y_original, sm.add_constant(y_retardada)).fit()

# Extract parameters
beta = modelo.params["x1"]  # Coeficiente
var_beta = modelo.bse["x1"] ** 2  # Variance del coeficiente

# Compute Durbin's h statistic
h = (1 - dw / 2) * np.sqrt(n / (1 - var_beta))


print("h-Durbin:", h)

# Check significance of h
if np.abs(h) > 1.96:  # Rough 95% confidence interval
    print("Autocorrelation detectada.")
else:
    print("Autocorrelation no significativa.")

"""## Ljung-Box"""

from statsmodels.stats.diagnostic import acorr_ljungbox


acorr_ljungbox(mcp.resid, lags=5)

"""## Corrección: Cochran-Orcutt"""

rho= 1 - dw/2 # dw = 2(1-rho) => rho = 1 - DW/2
print(rho)
mco_autocorr=sm.GLSAR(y, sm.add_constant(X), rho=rho)
res=mco_autocorr.iterative_fit(maxiter=100,rtol=10**(-10))


print ('Iteraciones = %d -->  Converge: %s' % (res.iter, res.converged) )
print ('Rho =  ', mco_autocorr.rho)
print(res.summary())